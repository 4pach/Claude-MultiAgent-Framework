#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimizer AI - –ò–ò-—Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ MCP –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
–ß–∞—Å—Ç—å Claude MultiAgent Framework

–ê–≤—Ç–æ—Ä: Claude MultiAgent System
–î–∞—Ç–∞: 2025-07-11
"""

import json
import sqlite3
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import statistics
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
import pickle

@dataclass
class OptimizationRecommendation:
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    id: str
    category: str
    priority: int  # 1-5, –≥–¥–µ 5 = –∫—Ä–∏—Ç–∏—á–Ω–æ
    title: str
    description: str
    impact_estimate: str
    implementation_complexity: str
    estimated_savings: Dict[str, float]  # tokens, time, etc.
    agent: Optional[str]
    mcp_server: Optional[str]
    confidence_score: float
    generated_at: datetime

@dataclass
class PerformancePattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    pattern_type: str
    frequency: float
    impact_score: float
    description: str
    affected_components: List[str]
    recommendations: List[str]

class OptimizerAI:
    """–ò–ò-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, models_dir: str = "recommendations/models"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)
        
        # –ü—É—Ç–∏ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö
        self.performance_db = Path("monitoring/performance.db")
        self.cache_db = Path("monitoring/cache/cache_metadata.db")
        
        # –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.anomaly_detector = None
        self.pattern_clusterer = None
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.thresholds = {
            'slow_response': 5.0,
            'high_tokens': 800,
            'low_success_rate': 0.9,
            'poor_cache_rate': 0.3,
            'anomaly_threshold': -0.5
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.load_or_train_models()
    
    def load_or_train_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ML"""
        anomaly_model_path = self.models_dir / "anomaly_detector.pkl"
        cluster_model_path = self.models_dir / "pattern_clusterer.pkl"
        
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
            if anomaly_model_path.exists():
                with open(anomaly_model_path, 'rb') as f:
                    self.anomaly_detector = pickle.load(f)
            
            if cluster_model_path.exists():
                with open(cluster_model_path, 'rb') as f:
                    self.pattern_clusterer = pickle.load(f)
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –æ–±—É—á–∞–µ–º –Ω–æ–≤—ã–µ
            if self.anomaly_detector is None or self.pattern_clusterer is None:
                self.train_models()
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.train_models()
    
    def train_models(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        print("üß† –û–±—É—á–µ–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        training_data = self.get_training_data()
        
        if len(training_data) < 10:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
            # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
            self.pattern_clusterer = KMeans(n_clusters=5, random_state=42)
            return
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        features = np.array(training_data)
        
        # –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.anomaly_detector.fit(features)
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        n_clusters = min(5, max(2, len(training_data) // 5))
        self.pattern_clusterer = KMeans(n_clusters=n_clusters, random_state=42)
        self.pattern_clusterer.fit(features)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.save_models()
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            with open(self.models_dir / "anomaly_detector.pkl", 'wb') as f:
                pickle.dump(self.anomaly_detector, f)
            
            with open(self.models_dir / "pattern_clusterer.pkl", 'wb') as f:
                pickle.dump(self.pattern_clusterer, f)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def get_training_data(self) -> List[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        if not self.performance_db.exists():
            return []
        
        try:
            with sqlite3.connect(self.performance_db) as conn:
                cursor = conn.execute('''
                    SELECT response_time, tokens_used, success, 
                           strftime('%H', timestamp) as hour,
                           length(query_hash) as query_complexity
                    FROM performance_records
                    WHERE timestamp > datetime('now', '-30 days')
                    ORDER BY timestamp DESC
                    LIMIT 1000
                ''')
                
                data = []
                for row in cursor.fetchall():
                    response_time, tokens, success, hour, complexity = row
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    features = [
                        min(response_time / 20.0, 1.0),  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
                        min(tokens / 1500.0, 1.0),       # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤
                        float(success),                   # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å (0 –∏–ª–∏ 1)
                        int(hour) / 24.0,                # –ß–∞—Å –¥–Ω—è (0-1)
                        min(complexity / 50.0, 1.0)      # –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                    ]
                    data.append(features)
                
                return data
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            return []
    
    def detect_anomalies(self, recent_hours: int = 24) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if self.anomaly_detector is None:
            return []
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        cutoff_time = datetime.now() - timedelta(hours=recent_hours)
        recent_data = self.get_recent_performance_data(cutoff_time)
        
        if len(recent_data) < 5:
            return []
        
        anomalies = []
        
        for record in recent_data:
            features = np.array([record['features']]).reshape(1, -1)
            anomaly_score = self.anomaly_detector.decision_function(features)[0]
            
            if anomaly_score < self.thresholds['anomaly_threshold']:
                anomalies.append({
                    'timestamp': record['timestamp'],
                    'agent': record['agent'],
                    'mcp_server': record['mcp_server'],
                    'anomaly_score': anomaly_score,
                    'features': record['original_features'],
                    'description': self.describe_anomaly(record['original_features'])
                })
        
        return sorted(anomalies, key=lambda x: x['anomaly_score'])
    
    def get_recent_performance_data(self, cutoff_time: datetime) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.performance_db.exists():
            return []
        
        try:
            with sqlite3.connect(self.performance_db) as conn:
                cursor = conn.execute('''
                    SELECT timestamp, agent, mcp_server, response_time, 
                           tokens_used, success, query_hash
                    FROM performance_records
                    WHERE timestamp > ?
                    ORDER BY timestamp DESC
                ''', (cutoff_time.isoformat(),))
                
                data = []
                for row in cursor.fetchall():
                    timestamp, agent, server, response_time, tokens, success, query_hash = row
                    
                    original_features = {
                        'response_time': response_time,
                        'tokens_used': tokens,
                        'success': success,
                        'query_complexity': len(query_hash)
                    }
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
                    hour = datetime.fromisoformat(timestamp).hour
                    features = [
                        min(response_time / 20.0, 1.0),
                        min(tokens / 1500.0, 1.0),
                        float(success),
                        hour / 24.0,
                        min(len(query_hash) / 50.0, 1.0)
                    ]
                    
                    data.append({
                        'timestamp': timestamp,
                        'agent': agent,
                        'mcp_server': server,
                        'original_features': original_features,
                        'features': features
                    })
                
                return data
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []
    
    def describe_anomaly(self, features: Dict) -> str:
        """–û–ø–∏—Å–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–π –∞–Ω–æ–º–∞–ª–∏–∏"""
        descriptions = []
        
        if features['response_time'] > 15.0:
            descriptions.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ({features['response_time']:.1f}—Å–µ–∫)")
        elif features['response_time'] > 10.0:
            descriptions.append(f"–ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ({features['response_time']:.1f}—Å–µ–∫)")
        
        if features['tokens_used'] > 1200:
            descriptions.append(f"–ß—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ ({features['tokens_used']})")
        
        if not features['success']:
            descriptions.append("–ù–µ—É—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
        
        if not descriptions:
            descriptions.append("–ù–µ–æ–±—ã—á–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        return "; ".join(descriptions)
    
    def identify_patterns(self) -> List[PerformancePattern]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if self.pattern_clusterer is None:
            return []
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        cutoff_time = datetime.now() - timedelta(days=7)
        recent_data = self.get_recent_performance_data(cutoff_time)
        
        if len(recent_data) < 10:
            return []
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        features_matrix = np.array([record['features'] for record in recent_data])
        cluster_labels = self.pattern_clusterer.predict(features_matrix)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        patterns = []
        for cluster_id in np.unique(cluster_labels):
            cluster_data = [record for i, record in enumerate(recent_data) 
                          if cluster_labels[i] == cluster_id]
            
            if len(cluster_data) < 3:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                continue
            
            pattern = self.analyze_cluster(cluster_id, cluster_data)
            if pattern:
                patterns.append(pattern)
        
        return patterns
    
    def analyze_cluster(self, cluster_id: int, cluster_data: List[Dict]) -> Optional[PerformancePattern]:
        """–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        if len(cluster_data) < 3:
            return None
        
        # –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞
        response_times = [d['original_features']['response_time'] for d in cluster_data]
        tokens = [d['original_features']['tokens_used'] for d in cluster_data]
        success_rates = [d['original_features']['success'] for d in cluster_data]
        
        avg_response_time = statistics.mean(response_times)
        avg_tokens = statistics.mean(tokens)
        success_rate = statistics.mean(success_rates)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        pattern_type = self.classify_pattern(avg_response_time, avg_tokens, success_rate)
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        agents = set(d['agent'] for d in cluster_data)
        servers = set(d['mcp_server'] for d in cluster_data)
        
        # –ß–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
        frequency = len(cluster_data) / len(cluster_data)  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        
        # –û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è
        impact_score = self.calculate_impact_score(avg_response_time, avg_tokens, success_rate)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        recommendations = self.generate_pattern_recommendations(
            pattern_type, agents, servers, avg_response_time, avg_tokens, success_rate
        )
        
        return PerformancePattern(
            pattern_type=pattern_type,
            frequency=frequency,
            impact_score=impact_score,
            description=self.describe_pattern(pattern_type, avg_response_time, avg_tokens, success_rate),
            affected_components=list(agents | servers),
            recommendations=recommendations
        )
    
    def classify_pattern(self, response_time: float, tokens: float, success_rate: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        if success_rate < 0.8:
            return "high_failure_rate"
        elif response_time > 10.0:
            return "slow_responses"
        elif tokens > 1000:
            return "high_token_consumption"
        elif response_time < 2.0 and tokens < 200:
            return "efficient_operations"
        elif response_time > 5.0 and tokens > 600:
            return "resource_intensive"
        else:
            return "normal_operations"
    
    def calculate_impact_score(self, response_time: float, tokens: float, success_rate: float) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ (0-1, –≥–¥–µ 1 = –ø–ª–æ—Ö–æ)
        time_impact = min(response_time / 20.0, 1.0)
        token_impact = min(tokens / 1500.0, 1.0)
        success_impact = 1.0 - success_rate
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        return (time_impact * 0.4 + token_impact * 0.3 + success_impact * 0.3) * 100
    
    def describe_pattern(self, pattern_type: str, response_time: float, 
                        tokens: float, success_rate: float) -> str:
        """–û–ø–∏—Å–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        descriptions = {
            "high_failure_rate": f"–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á (—É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%})",
            "slow_responses": f"–ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (—Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {response_time:.1f}—Å–µ–∫)",
            "high_token_consumption": f"–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ: {tokens:.0f})",
            "efficient_operations": f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–≤—Ä–µ–º—è: {response_time:.1f}—Å–µ–∫, —Ç–æ–∫–µ–Ω—ã: {tokens:.0f})",
            "resource_intensive": f"–†–µ—Å—É—Ä—Å–æ–µ–º–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–≤—Ä–µ–º—è: {response_time:.1f}—Å–µ–∫, —Ç–æ–∫–µ–Ω—ã: {tokens:.0f})",
            "normal_operations": f"–û–±—ã—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–≤—Ä–µ–º—è: {response_time:.1f}—Å–µ–∫, —Ç–æ–∫–µ–Ω—ã: {tokens:.0f})"
        }
        
        return descriptions.get(pattern_type, f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern_type}")
    
    def generate_pattern_recommendations(self, pattern_type: str, agents: set, servers: set,
                                       response_time: float, tokens: float, 
                                       success_rate: float) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        recommendations = []
        
        if pattern_type == "high_failure_rate":
            recommendations.extend([
                f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–æ–≤: {', '.join(servers)}",
                "–î–æ–±–∞–≤–∏—Ç—å retry –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤",
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏ –æ—à–∏–±–æ–∫ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω"
            ])
        
        elif pattern_type == "slow_responses":
            recommendations.extend([
                f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Å–µ—Ä–≤–µ—Ä–æ–≤: {', '.join(servers)}",
                "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤",
                "–£–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç—ã –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å –±–æ–ª—å—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã"
            ])
        
        elif pattern_type == "high_token_consumption":
            recommendations.extend([
                f"–°—É–∑–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: {', '.join(agents)}",
                "–£–ª—É—á—à–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                "–£–≤–µ–ª–∏—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"
            ])
        
        elif pattern_type == "resource_intensive":
            recommendations.extend([
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è",
                "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤",
                f"–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–∏–º–∏—Ç—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: {', '.join(agents)}"
            ])
        
        elif pattern_type == "efficient_operations":
            recommendations.extend([
                "–ò–∑—É—á–∏—Ç—å –∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç—å —É—Å–ø–µ—à–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω",
                "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫ –¥—Ä—É–≥–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º"
            ])
        
        return recommendations
    
    def generate_comprehensive_recommendations(self) -> List[OptimizationRecommendation]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        anomalies = self.detect_anomalies(24)
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = self.identify_patterns()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        stats_recommendations = self.generate_statistical_recommendations()
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        rec_id = 1
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–æ–º–∞–ª–∏–π
        for anomaly in anomalies[:5]:  # –¢–æ–ø-5 –∞–Ω–æ–º–∞–ª–∏–π
            recommendations.append(OptimizationRecommendation(
                id=f"ANOM_{rec_id:03d}",
                category="anomaly",
                priority=4,
                title=f"–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ {anomaly['mcp_server']}",
                description=f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è: {anomaly['description']}",
                impact_estimate="–í—ã—Å–æ–∫–∏–π",
                implementation_complexity="–°—Ä–µ–¥–Ω—è—è",
                estimated_savings={"response_time": 2.0, "tokens": 100},
                agent=anomaly['agent'],
                mcp_server=anomaly['mcp_server'],
                confidence_score=abs(anomaly['anomaly_score']),
                generated_at=datetime.now()
            ))
            rec_id += 1
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in patterns:
            if pattern.impact_score > 50:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                priority = 5 if pattern.impact_score > 80 else 3
                
                recommendations.append(OptimizationRecommendation(
                    id=f"PATT_{rec_id:03d}",
                    category="pattern",
                    priority=priority,
                    title=f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {pattern.pattern_type}",
                    description=pattern.description,
                    impact_estimate="–í—ã—Å–æ–∫–∏–π" if pattern.impact_score > 70 else "–°—Ä–µ–¥–Ω–∏–π",
                    implementation_complexity="–°—Ä–µ–¥–Ω—è—è",
                    estimated_savings=self.estimate_pattern_savings(pattern),
                    agent=None,
                    mcp_server=None,
                    confidence_score=pattern.frequency,
                    generated_at=datetime.now()
                ))
                rec_id += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        for stat_rec in stats_recommendations:
            recommendations.append(OptimizationRecommendation(
                id=f"STAT_{rec_id:03d}",
                category="statistics",
                priority=stat_rec['priority'],
                title=stat_rec['title'],
                description=stat_rec['description'],
                impact_estimate=stat_rec['impact'],
                implementation_complexity=stat_rec['complexity'],
                estimated_savings=stat_rec['savings'],
                agent=stat_rec.get('agent'),
                mcp_server=stat_rec.get('mcp_server'),
                confidence_score=stat_rec['confidence'],
                generated_at=datetime.now()
            ))
            rec_id += 1
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ confidence score
        recommendations.sort(key=lambda x: (x.priority, x.confidence_score), reverse=True)
        
        return recommendations[:15]  # –¢–æ–ø-15 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    def estimate_pattern_savings(self, pattern: PerformancePattern) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ —ç–∫–æ–Ω–æ–º–∏–∏ –æ—Ç —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        savings = {"response_time": 0.0, "tokens": 0, "error_reduction": 0.0}
        
        if pattern.pattern_type == "slow_responses":
            savings["response_time"] = pattern.impact_score * 0.1
        elif pattern.pattern_type == "high_token_consumption":
            savings["tokens"] = pattern.impact_score * 5
        elif pattern.pattern_type == "high_failure_rate":
            savings["error_reduction"] = pattern.impact_score * 0.01
        
        return savings
    
    def generate_statistical_recommendations(self) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        cutoff_time = datetime.now() - timedelta(days=7)
        
        try:
            with sqlite3.connect(self.performance_db) as conn:
                # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å–µ—Ä–≤–µ—Ä–∞–º
                cursor = conn.execute('''
                    SELECT mcp_server, AVG(response_time), AVG(tokens_used), 
                           AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END), COUNT(*)
                    FROM performance_records
                    WHERE timestamp > ?
                    GROUP BY mcp_server
                    HAVING COUNT(*) >= 5
                    ORDER BY AVG(response_time) DESC
                ''', (cutoff_time.isoformat(),))
                
                for row in cursor.fetchall():
                    server, avg_time, avg_tokens, success_rate, count = row
                    
                    if avg_time > self.thresholds['slow_response']:
                        recommendations.append({
                            'priority': 4,
                            'title': f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞ {server}",
                            'description': f"–°–µ—Ä–≤–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã: {avg_time:.1f}—Å–µ–∫ –≤ —Å—Ä–µ–¥–Ω–µ–º",
                            'impact': "–í—ã—Å–æ–∫–∏–π",
                            'complexity': "–°—Ä–µ–¥–Ω—è—è",
                            'savings': {"response_time": avg_time - 3.0, "tokens": 0},
                            'mcp_server': server,
                            'confidence': min(count / 50.0, 1.0)
                        })
                    
                    if success_rate < self.thresholds['low_success_rate']:
                        recommendations.append({
                            'priority': 5,
                            'title': f"–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ {server}",
                            'description': f"–ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%}",
                            'impact': "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π",
                            'complexity': "–í—ã—Å–æ–∫–∞—è",
                            'savings': {"error_reduction": 0.9 - success_rate, "tokens": 0},
                            'mcp_server': server,
                            'confidence': min(count / 30.0, 1.0)
                        })
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ –∞–≥–µ–Ω—Ç–∞–º
                cursor = conn.execute('''
                    SELECT agent, AVG(response_time), AVG(tokens_used), COUNT(*)
                    FROM performance_records
                    WHERE timestamp > ?
                    GROUP BY agent
                    HAVING COUNT(*) >= 5
                    ORDER BY AVG(tokens_used) DESC
                ''', (cutoff_time.isoformat(),))
                
                for row in cursor.fetchall():
                    agent, avg_time, avg_tokens, count = row
                    
                    if avg_tokens > self.thresholds['high_tokens']:
                        recommendations.append({
                            'priority': 3,
                            'title': f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent}",
                            'description': f"–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: {avg_tokens:.0f} –≤ —Å—Ä–µ–¥–Ω–µ–º",
                            'impact': "–°—Ä–µ–¥–Ω–∏–π",
                            'complexity': "–ù–∏–∑–∫–∞—è",
                            'savings': {"tokens": avg_tokens - 500, "response_time": 0},
                            'agent': agent,
                            'confidence': min(count / 40.0, 1.0)
                        })
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return recommendations
    
    def save_recommendations(self, recommendations: List[OptimizationRecommendation]) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ —Ñ–∞–π–ª"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ai_recommendations_{timestamp}.json"
        filepath = Path("recommendations") / filename
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        filepath.parent.mkdir(exist_ok=True)
        
        # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations_data = {
            'generated_at': datetime.now().isoformat(),
            'total_recommendations': len(recommendations),
            'recommendations': [asdict(rec) for rec in recommendations]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(recommendations_data, f, ensure_ascii=False, indent=2, default=str)
        
        return str(filepath)
    
    def print_recommendations_summary(self, recommendations: List[OptimizationRecommendation]):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        if not recommendations:
            print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: –í—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ!")
            return
        
        print("\nüí° === –ò–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò ===")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        priority_groups = {}
        for rec in recommendations:
            priority_groups.setdefault(rec.priority, []).append(rec)
        
        priority_names = {5: "üî¥ –ö—Ä–∏—Ç–∏—á–Ω–æ", 4: "üß° –í—ã—Å–æ–∫–∏–π", 3: "üü° –°—Ä–µ–¥–Ω–∏–π", 2: "üü¢ –ù–∏–∑–∫–∏–π", 1: "üîµ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π"}
        
        for priority in sorted(priority_groups.keys(), reverse=True):
            group_recs = priority_groups[priority]
            print(f"\n{priority_names.get(priority, f'–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç {priority}')} ({len(group_recs)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π):")
            
            for rec in group_recs[:3]:  # –¢–æ–ø-3 –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
                print(f"  ‚Ä¢ {rec.title}")
                print(f"    {rec.description}")
                print(f"    –í–ª–∏—è–Ω–∏–µ: {rec.impact_estimate}, –°–ª–æ–∂–Ω–æ—Å—Ç—å: {rec.implementation_complexity}")
                
                if rec.estimated_savings:
                    savings_str = ", ".join([
                        f"{k}: {v:.1f}" for k, v in rec.estimated_savings.items() if v > 0
                    ])
                    if savings_str:
                        print(f"    –≠–∫–æ–Ω–æ–º–∏—è: {savings_str}")
                print()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time_savings = sum(rec.estimated_savings.get('response_time', 0) for rec in recommendations)
        total_token_savings = sum(rec.estimated_savings.get('tokens', 0) for rec in recommendations)
        
        print(f"üìä –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è:")
        print(f"  ‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {total_time_savings:.1f}—Å–µ–∫")
        print(f"  üî¢ –¢–æ–∫–µ–Ω—ã: {total_token_savings:.0f}")
        print("="*50)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ò–ò-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
optimizer_ai = OptimizerAI()

def generate_ai_recommendations() -> List[OptimizationRecommendation]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ò–ò-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    return optimizer_ai.generate_comprehensive_recommendations()

def detect_performance_anomalies(hours: int = 24) -> List[Dict]:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    return optimizer_ai.detect_anomalies(hours)

def identify_performance_patterns() -> List[PerformancePattern]:
    """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    return optimizer_ai.identify_patterns()

def save_ai_recommendations(recommendations: List[OptimizationRecommendation]) -> str:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ò–ò-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    return optimizer_ai.save_recommendations(recommendations)

def print_ai_recommendations_summary(recommendations: List[OptimizationRecommendation]):
    """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –ò–ò-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    optimizer_ai.print_recommendations_summary(recommendations)

if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ò–ò-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    print("üß† AI Optimizer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    recommendations = generate_ai_recommendations()
    
    if recommendations:
        print_ai_recommendations_summary(recommendations)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        saved_file = save_ai_recommendations(recommendations)
        print(f"üíæ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {saved_file}")
    else:
        print("üí° –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")