#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Performance Tracker - –¢—Ä–µ–∫–µ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MCP
–ß–∞—Å—Ç—å Claude MultiAgent Framework

–ê–≤—Ç–æ—Ä: Claude MultiAgent System  
–î–∞—Ç–∞: 2025-07-11
"""

import json
import sqlite3
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import statistics
import asyncio

@dataclass
class PerformanceRecord:
    """–ó–∞–ø–∏—Å—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    timestamp: datetime
    agent: str
    mcp_server: str
    query_hash: str
    query_length: int
    response_time: float
    tokens_used: int
    success: bool
    cached: bool
    session_id: str

@dataclass
class TrendAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    server_name: str
    avg_response_time: float
    avg_tokens: float
    success_rate: float
    cache_hit_rate: float
    usage_frequency: int
    trend_direction: str  # 'improving', 'degrading', 'stable'
    efficiency_score: float

class PerformanceTracker:
    """–¢—Ä–µ–∫–µ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤"""
    
    def __init__(self, db_path: str = "monitoring/performance.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True)
        self.session_id = f"session_{int(datetime.now().timestamp())}"
        self.lock = threading.Lock()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        self.init_database()
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.performance_thresholds = {
            'excellent_response_time': 2.0,
            'good_response_time': 5.0,
            'poor_response_time': 10.0,
            'efficient_tokens': 500,
            'verbose_tokens': 1000,
            'minimum_cache_rate': 0.3,
            'excellent_cache_rate': 0.7
        }
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS performance_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    agent TEXT NOT NULL,
                    mcp_server TEXT NOT NULL,
                    query_hash TEXT NOT NULL,
                    query_length INTEGER NOT NULL,
                    response_time REAL NOT NULL,
                    tokens_used INTEGER NOT NULL,
                    success BOOLEAN NOT NULL,
                    cached BOOLEAN NOT NULL,
                    session_id TEXT NOT NULL
                )
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_timestamp ON performance_records(timestamp)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_mcp_server ON performance_records(mcp_server)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_session ON performance_records(session_id)
            ''')
    
    def record_performance(self, agent: str, mcp_server: str, query: str,
                          response_time: float, tokens_used: int, 
                          success: bool, cached: bool = False):
        """–ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        record = PerformanceRecord(
            timestamp=datetime.now(),
            agent=agent,
            mcp_server=mcp_server,
            query_hash=str(hash(query)),
            query_length=len(query),
            response_time=response_time,
            tokens_used=tokens_used,
            success=success,
            cached=cached,
            session_id=self.session_id
        )
        
        with self.lock:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT INTO performance_records 
                    (timestamp, agent, mcp_server, query_hash, query_length,
                     response_time, tokens_used, success, cached, session_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    record.timestamp.isoformat(),
                    record.agent,
                    record.mcp_server,
                    record.query_hash,
                    record.query_length,
                    record.response_time,
                    record.tokens_used,
                    record.success,
                    record.cached,
                    record.session_id
                ))
    
    def get_server_performance(self, server_name: str, days: int = 7) -> TrendAnalysis:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT response_time, tokens_used, success, cached
                FROM performance_records
                WHERE mcp_server = ? AND timestamp > ?
                ORDER BY timestamp DESC
            ''', (server_name, cutoff_date.isoformat()))
            
            records = cursor.fetchall()
        
        if not records:
            return TrendAnalysis(
                server_name=server_name,
                avg_response_time=0.0,
                avg_tokens=0.0,
                success_rate=0.0,
                cache_hit_rate=0.0,
                usage_frequency=0,
                trend_direction='no_data',
                efficiency_score=0.0
            )
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        response_times = [r[0] for r in records]
        token_counts = [r[1] for r in records]
        successes = [r[2] for r in records]
        cached = [r[3] for r in records]
        
        avg_response_time = statistics.mean(response_times)
        avg_tokens = statistics.mean(token_counts)
        success_rate = sum(successes) / len(successes)
        cache_hit_rate = sum(cached) / len(cached)
        usage_frequency = len(records)
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–π –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–ª–æ–≤–∏–Ω—ã –ø–µ—Ä–∏–æ–¥–∞)
        mid_point = len(records) // 2
        if mid_point > 0:
            early_avg = statistics.mean(response_times[mid_point:])
            recent_avg = statistics.mean(response_times[:mid_point])
            
            if recent_avg < early_avg * 0.9:
                trend_direction = 'improving'
            elif recent_avg > early_avg * 1.1:
                trend_direction = 'degrading'
            else:
                trend_direction = 'stable'
        else:
            trend_direction = 'insufficient_data'
        
        # –†–∞—Å—á–µ—Ç efficiency score (0-100)
        efficiency_score = self.calculate_efficiency_score(
            avg_response_time, avg_tokens, success_rate, cache_hit_rate
        )
        
        return TrendAnalysis(
            server_name=server_name,
            avg_response_time=avg_response_time,
            avg_tokens=avg_tokens,
            success_rate=success_rate,
            cache_hit_rate=cache_hit_rate,
            usage_frequency=usage_frequency,
            trend_direction=trend_direction,
            efficiency_score=efficiency_score
        )
    
    def calculate_efficiency_score(self, response_time: float, tokens: float,
                                 success_rate: float, cache_hit_rate: float) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (0-100)"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ (0-1)
        time_score = max(0, min(1, (10 - response_time) / 10))
        token_score = max(0, min(1, (1000 - tokens) / 1000))
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        efficiency = (
            time_score * 0.3 +           # –°–∫–æ—Ä–æ—Å—Ç—å - 30%
            token_score * 0.25 +         # –≠–∫–æ–Ω–æ–º–∏—á–Ω–æ—Å—Ç—å - 25%
            success_rate * 0.25 +        # –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å - 25%
            cache_hit_rate * 0.2         # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ - 20%
        )
        
        return round(efficiency * 100, 1)
    
    def get_agent_performance_summary(self, days: int = 7) -> Dict[str, Dict]:
        """–°–≤–æ–¥–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∞–≥–µ–Ω—Ç–∞–º"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT agent, AVG(response_time), AVG(tokens_used), 
                       AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END),
                       AVG(CASE WHEN cached THEN 1.0 ELSE 0.0 END),
                       COUNT(*)
                FROM performance_records
                WHERE timestamp > ?
                GROUP BY agent
                ORDER BY COUNT(*) DESC
            ''', (cutoff_date.isoformat(),))
            
            results = {}
            for row in cursor.fetchall():
                agent, avg_time, avg_tokens, success_rate, cache_rate, count = row
                results[agent] = {
                    'avg_response_time': round(avg_time, 2),
                    'avg_tokens': round(avg_tokens, 1),
                    'success_rate': round(success_rate, 3),
                    'cache_hit_rate': round(cache_rate, 3),
                    'usage_count': count,
                    'efficiency_score': self.calculate_efficiency_score(
                        avg_time, avg_tokens, success_rate, cache_rate
                    )
                }
        
        return results
    
    def identify_performance_anomalies(self, days: int = 7) -> List[Dict]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        anomalies = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        with sqlite3.connect(self.db_path) as conn:
            # –ü–æ–∏—Å–∫ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor = conn.execute('''
                SELECT timestamp, agent, mcp_server, response_time, tokens_used
                FROM performance_records
                WHERE timestamp > ? AND response_time > ?
                ORDER BY response_time DESC
                LIMIT 10
            ''', (cutoff_date.isoformat(), self.performance_thresholds['poor_response_time']))
            
            for row in cursor.fetchall():
                anomalies.append({
                    'type': 'slow_response',
                    'timestamp': row[0],
                    'agent': row[1],
                    'server': row[2],
                    'response_time': row[3],
                    'tokens': row[4],
                    'severity': 'high' if row[3] > 15 else 'medium'
                })
            
            # –ü–æ–∏—Å–∫ —á—Ä–µ–∑–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω-–ø–æ—Ç—Ä–µ–±–ª—è—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor = conn.execute('''
                SELECT timestamp, agent, mcp_server, response_time, tokens_used
                FROM performance_records
                WHERE timestamp > ? AND tokens_used > ?
                ORDER BY tokens_used DESC
                LIMIT 10
            ''', (cutoff_date.isoformat(), self.performance_thresholds['verbose_tokens']))
            
            for row in cursor.fetchall():
                anomalies.append({
                    'type': 'high_token_usage',
                    'timestamp': row[0],
                    'agent': row[1],
                    'server': row[2],
                    'response_time': row[3],
                    'tokens': row[4],
                    'severity': 'medium' if row[4] < 1500 else 'high'
                })
            
            # –ü–æ–∏—Å–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–∏–º success rate
            cursor = conn.execute('''
                SELECT mcp_server, AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate,
                       COUNT(*) as total_requests
                FROM performance_records
                WHERE timestamp > ?
                GROUP BY mcp_server
                HAVING success_rate < 0.9 AND total_requests >= 3
                ORDER BY success_rate ASC
            ''', (cutoff_date.isoformat(),))
            
            for row in cursor.fetchall():
                anomalies.append({
                    'type': 'low_success_rate',
                    'server': row[0],
                    'success_rate': round(row[1], 3),
                    'total_requests': row[2],
                    'severity': 'high' if row[1] < 0.7 else 'medium'
                })
        
        return anomalies
    
    def generate_optimization_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤
        servers = self.get_all_servers()
        for server in servers:
            analysis = self.get_server_performance(server)
            
            if analysis.efficiency_score < 50:
                recommendations.append(
                    f"üî¥ {server}: –Ω–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å ({analysis.efficiency_score}%), "
                    f"—Ç—Ä–µ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
                )
            
            if analysis.avg_response_time > self.performance_thresholds['poor_response_time']:
                recommendations.append(
                    f"‚è∞ {server}: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ({analysis.avg_response_time:.1f}—Å–µ–∫), "
                    f"—Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–ø—Ä–æ—â–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤"
                )
            
            if analysis.cache_hit_rate < self.performance_thresholds['minimum_cache_rate']:
                recommendations.append(
                    f"üíæ {server}: –Ω–∏–∑–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ ({analysis.cache_hit_rate:.1%}), "
                    f"—É–≤–µ–ª–∏—á–∏—Ç—å TTL –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"
                )
            
            if analysis.trend_direction == 'degrading':
                recommendations.append(
                    f"üìâ {server}: —É—Ö—É–¥—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, "
                    f"—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
                )
        
        # –ê–Ω–∞–ª–∏–∑ –∞–≥–µ–Ω—Ç–æ–≤
        agent_stats = self.get_agent_performance_summary()
        for agent, stats in agent_stats.items():
            if stats['efficiency_score'] < 60:
                recommendations.append(
                    f"üë§ –ê–≥–µ–Ω—Ç {agent}: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å {stats['efficiency_score']}%, "
                    f"–ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MCP"
                )
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        anomalies = self.identify_performance_anomalies()
        high_severity_anomalies = [a for a in anomalies if a.get('severity') == 'high']
        
        if len(high_severity_anomalies) > 3:
            recommendations.append(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(high_severity_anomalies)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–Ω–æ–º–∞–ª–∏–π, "
                f"—Ç—Ä–µ–±—É–µ—Ç—Å—è —Å—Ä–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
            )
        
        return recommendations
    
    def get_all_servers(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT DISTINCT mcp_server FROM performance_records
                ORDER BY mcp_server
            ''')
            return [row[0] for row in cursor.fetchall()]
    
    def export_performance_report(self, filepath: str = None) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"reports/performance_report_{timestamp}.json"
        
        Path("reports").mkdir(exist_ok=True)
        
        # –°–±–æ—Ä –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        report_data = {
            'generated_at': datetime.now().isoformat(),
            'analysis_period_days': 7,
            'server_analyses': {},
            'agent_performance': self.get_agent_performance_summary(),
            'anomalies': self.identify_performance_anomalies(),
            'recommendations': self.generate_optimization_recommendations()
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å–µ—Ä–≤–µ—Ä–∞–º
        for server in self.get_all_servers():
            analysis = self.get_server_performance(server)
            report_data['server_analyses'][server] = asdict(analysis)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        return filepath
    
    def get_session_statistics(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT COUNT(*), AVG(response_time), SUM(tokens_used),
                       AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END),
                       AVG(CASE WHEN cached THEN 1.0 ELSE 0.0 END)
                FROM performance_records
                WHERE session_id = ?
            ''', (self.session_id,))
            
            row = cursor.fetchone()
            if row[0] == 0:
                return {
                    'total_requests': 0,
                    'avg_response_time': 0.0,
                    'total_tokens': 0,
                    'success_rate': 0.0,
                    'cache_hit_rate': 0.0
                }
            
            return {
                'total_requests': row[0],
                'avg_response_time': round(row[1], 2),
                'total_tokens': row[2],
                'success_rate': round(row[3], 3),
                'cache_hit_rate': round(row[4], 3)
            }
    
    def cleanup_old_records(self, days_to_keep: int = 30):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                DELETE FROM performance_records
                WHERE timestamp < ?
            ''', (cutoff_date.isoformat(),))
            
            deleted_count = cursor.rowcount
            
        return deleted_count

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç—Ä–µ–∫–µ—Ä–∞
performance_tracker = PerformanceTracker()

def record_mcp_performance(agent: str, mcp_server: str, query: str,
                          response_time: float, tokens_used: int, 
                          success: bool, cached: bool = False):
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø–∏—Å–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    performance_tracker.record_performance(
        agent, mcp_server, query, response_time, tokens_used, success, cached
    )

def get_performance_recommendations() -> List[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    return performance_tracker.generate_optimization_recommendations()

def export_performance_report(filepath: str = None) -> str:
    """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    return performance_tracker.export_performance_report(filepath)

if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
    tracker = PerformanceTracker()
    
    print("üìä Performance Tracker –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤...")
    
    # –ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞
    servers = tracker.get_all_servers()
    if servers:
        for server in servers:
            analysis = tracker.get_server_performance(server)
            print(f"\nüìà {server}:")
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {analysis.efficiency_score}%")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {analysis.avg_response_time:.1f}—Å–µ–∫")
            print(f"  –¢—Ä–µ–Ω–¥: {analysis.trend_direction}")
    else:
        print("‚ÑπÔ∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")